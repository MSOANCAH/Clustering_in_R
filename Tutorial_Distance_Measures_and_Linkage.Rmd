---
title: "Distance Measures & Linkage"
author: "David Ziganto"
date: "April 18, 2016"
output: html_document
---

Distance measures are fundamental to anomaly detection, classification, and clustering. Three terms associated with distance measures come up frequently: similarity, dissimilarity, and correlation. We will now explore these concepts in more detail. 

Here is a list of common distance measures:

* Euclidean
* Manhattan (aka taxicab)
* Minkowski
* Cosine
* Mahalanobis

#### Euclidean Distance
Euclidean distance computes the distance between two objects in Euclidean space. That space can be 1-dimensional, 2-dimensional, or even n-dimensional. Think of the distance between points as the crow flies, to use an analogy. Use the Pythagorean Theorem to calculate the distance. For example, say you walk three blocks North and four blocks East. Your Euclidean distance is five blocks.

#### Manhattan Distance
Manhattant distance computes the distance between two objects by traversing vertically and horizontally in a grid-based system. Think of walking in a city. For example, take the same scenrio above: you walk three blocks North and four blocks East. Your Manhattan distance is seven blocks. 

#### Minkowski Distance
Minkowski distance is the most general form for calculating distances in Euclidean space. Euclidean and Manhattan distance are special cases of Minkowski distance. Specifically, Mnahattan distance is calculated using a parameter value r=1 and Euclidean distance uses r=2. When r=Inf, the Minkowski distance calculates the supremum (least upper bound); it also goes by the names Chebyshev distance and Chessboard distance. 

#### Cosine Similarity
Cosine similarity calculates the angle between two vectors. It is a metric of orientation, not magnitude. 

#### Mahalanobis Distance
Mahalanobis distance computes the distance between two groups of an object. Since the previous statement may not be entirely clear, imagine we have two clusters. Say they are well separated, though they needn't be. The Mahalanobis distance is the distance between cluster 1 and cluster 2. More specifically, it is a measure of distance between a point and a distribution. It is a multi-dimensional generalization of measureing how many standard deviations away a ponit is from the mean of the distribution. Mahalanobis distance removes sevearl limitations of Euclidean space:

* Unitless 
* Scale invariant
* Corrects for correlations between features
* Can handle curved and linear decision boundaries

**Cons:** memory and time requirements grow quadratically with number of features.

## (Dis)similarity & Correlation
Similarity and dissimilarity are opposite sides of the same coin. 

Similarity, like its name suggests, describes how similar objects are to each other. Similarity scores range from 0 to 1, where 0 indicates no similarity and 1 indicates exact similarity. 

Dissimilarity, on the the other hand, describes how different objects are from each other. Dissimilarity scores range from 0 to infinity, where 0 indicates two objects are exactly alike and infinity indicates they couldn't be more different. 

It is possible to convert from similarity to dissimilarity and vice-versa. 

Correlation is a statistical technique that can show how strongly pairs of variables are related. It can only be used with continuous data, not categorical. Correlation scores range from -1 to 1, where -1 indicates two variables are inversely related, 0 indicates no relation, and 1 indicates direct relationship. Correlation does not measure the distance between objects. It merely tells how strongly or weekly the relationship is between variables.

## Linkage Types
Linkage measures the distance between a data item and a cluster or between two clusters. Since a data point can be treated as a cluster, the only thing yet to be determined is the linkage methods available. Let's explore the possibilities.

### Single-Linkage
Single-linkage measures the minimum of all pairwise distances between points in two clusters. In other words, find the points in each of the two clusters that are closest and calculate the distance. 

This linkage method tends to produce **long, loose clusters**.

### Complete-Linkage

Complete-linkage measures the maximum of all pairwise distances between points in two clusters. In other words, find the points in each of the two clustes that are farthest apart and calculate the distance.

This linkage method tends to produce **very tight clusters**.

### Average-Linkage

Average-linkage measures the distance between the mean of each cluster. In other words, find the mean nof all data items within each cluster and then calculate the distance between the clusters' means.
